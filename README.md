<div align="center">

# PERL: Parameter-Efficient Reinforcement Learning  
> A minimal, modular, and lightning-fast framework for PEFT + RL.
</div>

## ðŸ§© Supported Parameter-Efficient Methods

| Method | Status | Description |
| :--- | :--- | :--- |
| **LoRA** | âœ… | Standard Low-Rank Adaptation |
| **DoRA** | âœ… | Weight-decomposed Low-Rank Adaptation |
| **MiSS** | âœ… | Mixture of Sub-Spaces (Efficient shard-sharing structure) |
| **AdaLoRA** | âœ… | Adaptive budget allocation for rank-adaptive matrices |
| **LoRA+** | âœ… | Differentiated learning rates for improved adaptation dynamics |
| **rsLORA** | âœ… | Rank stabilization scaling factors |
| **PiSSA** | âœ… | Principal Singular values & Singular vectors Adaptation |
| **MiLORA** | âœ… | Minor Singular components initialization |
| **LORA-FA** | âœ… | Memory-efficient adaptation with frozen projection matrix A |
| **VeRA** | âœ… | Vector-based Random Matrix Adaptation |
| **LN Tuning** | âœ… | Parameter-efficient tuning on Layer Normalization layers |
| **$IA^3$** | âœ… | Infused Adapter by Inhibiting and Amplifying Inner Activations |

## Environment Setup

```
uv pip install -r requirements.txt
```

```
uv pip install flash-attn --no-cache-dir --no-build-isolation
python -c "import flash_attn" # verify
```

## Training

```
source [your virtual env]/bin/activate
bash scripts/openr1/dapo_full.sh # run a full RL
bash scripts/openr1/dapo_lora.sh # run a lora RL
```