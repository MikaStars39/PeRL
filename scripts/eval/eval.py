#!/usr/bin/env python3
# æœ¬è„šæœ¬ä¾æ®ç”¨æˆ·éœ€æ±‚ï¼šå®ç°è¯„æµ‹æµç¨‹ï¼ˆå‚æ•°è§£æã€æ¨¡å‹åˆå¹¶ã€å¯åŠ¨vLLMã€ç”Ÿæˆã€æ‰“åˆ†ã€ç¼“å­˜/æ¢å¤ã€æ—¥å¿—ã€é˜¶æ®µåŒ–æç¤ºã€æœ€ç»ˆç»Ÿè®¡ï¼‰ã€‚
# å®ç°æ–¹æ¡ˆï¼šä½¿ç”¨argparseè§£æå¸¸è§„ä¸--vllm-*é€ä¼ å‚æ•°ï¼Œå¿…è¦æ—¶åœ¨CPUä¸Šåˆå¹¶LoRAå¹¶ä¿å­˜ï¼›åå°å¯åŠ¨æ”¯æŒæ•°æ®å¹¶è¡Œçš„vLLMæœåŠ¡å™¨ï¼Œ
# è½®è¯¢åç«¯ç”Ÿæˆå¤šæ¬¡rolloutå¹¶ç¼“å­˜åˆ°æ–‡ä»¶ï¼Œéšåè°ƒç”¨score_responseæ±‡æ€»ä¸ºresult.jsonlï¼Œæœ€åæ–°å¢ä¸€ä¸ªç»Ÿè®¡é˜¶æ®µè¾“å‡ºavg@k/pass@kï¼Œ
# åŒæ—¶è®°å½•æ—¥å¿—å¹¶å°†stdout/stderrå†™å…¥latest_run.logï¼›é€šè¿‡é˜¶æ®µåŒ–æ—¥å¿—æ ‡æ˜ç¬¬å‡ é˜¶æ®µçš„å¼€å§‹/ç»“æŸï¼ˆå«emojiï¼‰ã€‚æœ¬ç‰ˆå¼ºåˆ¶ä¾èµ–vLLMä¸GPUï¼Œ
# æ–°å¢ --num-gpus å‚æ•°ç”¨äºè¿è¡Œå‰æ ¡éªŒå¯ç”¨ GPU æ•°ï¼ˆbash è„šæœ¬ä¸­è®¾ä¸º 8ï¼‰ï¼Œç¡®ä¿æŒ‰éœ€æ±‚ä½¿ç”¨å¤šå¡å¹¶è¡Œã€‚

import argparse
import asyncio
import atexit
import importlib.util
import json
import logging
import os
import signal
import subprocess
import sys
import threading
import time
import urllib.error
import urllib.request
from pathlib import Path
from typing import Any, Dict, Iterable, List, Tuple, Optional
import math

try:
    import aiohttp
except ImportError:
    raise ImportError("éœ€è¦å®‰è£… aiohttp: pip install aiohttp")

from datasets import load_dataset
from peft import PeftModel
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch


class StreamToLogger:
    """Redirect stdout/stderråˆ°loggerï¼Œç¡®ä¿è¾“å‡ºè¢«æ–‡ä»¶ä¸æ§åˆ¶å°åŒæ—¶è®°å½•ã€‚"""

    def __init__(self, logger: logging.Logger, level: int) -> None:
        self.logger = logger
        self.level = level
        self._buffer = ""

    def write(self, buffer: str) -> None:
        self._buffer += buffer
        while "\n" in self._buffer:
            line, self._buffer = self._buffer.split("\n", 1)
            self.logger.log(self.level, line)

    def flush(self) -> None:
        if self._buffer:
            self.logger.log(self.level, self._buffer)
            self._buffer = ""


def setup_logging(result_dir: Path) -> logging.Logger:
    result_dir.mkdir(parents=True, exist_ok=True)
    log_path = result_dir / "latest_run.log"

    for handler in logging.root.handlers[:]:
        logging.root.removeHandler(handler)

    logging.root.setLevel(logging.INFO)
    formatter = logging.Formatter("%(asctime)s | %(levelname)s | %(message)s")
    file_handler = logging.FileHandler(log_path, mode="w", encoding="utf-8")
    file_handler.setFormatter(formatter)
    console_handler = logging.StreamHandler(sys.__stdout__)
    console_handler.setFormatter(formatter)
    logging.root.addHandler(file_handler)
    logging.root.addHandler(console_handler)

    stdout_logger = logging.getLogger("stdout")
    stdout_logger.setLevel(logging.INFO)
    stdout_logger.propagate = True
    stderr_logger = logging.getLogger("stderr")
    stderr_logger.setLevel(logging.ERROR)
    stderr_logger.propagate = True
    sys.stdout = StreamToLogger(stdout_logger, logging.INFO)
    sys.stderr = StreamToLogger(stderr_logger, logging.ERROR)

    return logging.getLogger("eval_all")


class StageContext:
    """é˜¶æ®µåŒ–æ—¥å¿—ä¸Šä¸‹æ–‡ï¼Œæ ‡è®°å¼€å§‹/ç»“æŸå’Œå¤±è´¥åœºæ™¯ã€‚"""

    def __init__(
        self,
        logger: logging.Logger,
        stage_id: int,
        name: str,
        emoji_start: str = "ğŸš€",
        emoji_end: str = "ğŸ",
        emoji_fail: str = "ğŸ’¥",
    ) -> None:
        self.logger = logger
        self.stage_id = stage_id
        self.name = name
        self.emoji_start = emoji_start
        self.emoji_end = emoji_end
        self.emoji_fail = emoji_fail

    def __enter__(self) -> "StageContext":
        self.logger.info("%s ç¬¬%dé˜¶æ®µå¼€å§‹ï¼š%s", self.emoji_start, self.stage_id, self.name)
        return self

    def __exit__(self, exc_type, exc, tb) -> None:  # noqa: ANN001
        if exc_type is None:
            self.logger.info("%s ç¬¬%dé˜¶æ®µç»“æŸï¼š%s", self.emoji_end, self.stage_id, self.name)
        else:
            self.logger.error("%s ç¬¬%dé˜¶æ®µå¤±è´¥ï¼š%sï¼Œé”™è¯¯ï¼š%s", self.emoji_fail, self.stage_id, self.name, exc)


def parse_args() -> Tuple[argparse.Namespace, List[str], List[str]]:
    parser = argparse.ArgumentParser(description="è¯„æµ‹å…¥å£è„šæœ¬ï¼Œæ”¯æŒæ¨¡å‹åˆå¹¶ã€vLLMå¯åŠ¨ä¸å¤šæ•°æ®é›†è¯„æµ‹ã€‚")
    parser.add_argument("--result-dir", required=True, help="ä¸­é—´è¿‡ç¨‹ä¸ç»“æœè¾“å‡ºç›®å½•ã€‚")
    parser.add_argument("--model", required=True, help="åŸºç¡€æ¨¡å‹åç§°æˆ–è·¯å¾„ã€‚")
    parser.add_argument("--adapter", default="", help="LoRA/PEFT adapterè·¯å¾„ï¼Œç•™ç©ºè¡¨ç¤ºä¸åˆå¹¶ã€‚")
    parser.add_argument("--dataset", default="aime2024", help="è¦è¯„æµ‹çš„æ•°æ®é›†ç¼©å†™ï¼Œè‹±æ–‡é€—å·åˆ†éš”ï¼ˆå¦‚ï¼šaime2024ï¼‰ã€‚")
    parser.add_argument("--rollout-n", type=int, default=1, help="æ¯ä¸ªsampleç”Ÿæˆå¤šå°‘æ¬¡rolloutã€‚")
    parser.add_argument("--serve-port", type=int, default=8000, help="ç¬¬ä¸€ä¸ªvLLMåç«¯ç«¯å£å·ã€‚")
    parser.add_argument("--dp-size", type=int, default=1, help="æ•°æ®å¹¶è¡Œåç«¯æ•°é‡ï¼ˆå¯åŠ¨å¤šä¸ªvLLMï¼‰ã€‚")
    parser.add_argument("--tp-size", type=int, default=1, help="ä¼ ç»™vLLMçš„å¼ é‡å¹¶è¡Œå¤§å°ã€‚")
    parser.add_argument("--num-gpus", type=int, default=1, help="è¿è¡Œå‰æ ¡éªŒéœ€è¦çš„GPUæ•°é‡ï¼Œä¸è¶³åˆ™æŠ¥é”™ã€‚")
    parser.add_argument(
        "--gpu-memory-utilization",
        type=float,
        default=0.95,
        help="ä¼ ç»™vLLMçš„GPUæ˜¾å­˜åˆ©ç”¨ç‡ä¸Šé™ï¼ˆ0~1ï¼‰ï¼Œç”¨äºæ§åˆ¶å•å¡æ˜¾å­˜å ç”¨æ¯”ä¾‹ã€‚",
    )
    parser.add_argument("--temperature", type=float, default=1.0, help="ç”Ÿæˆæ¸©åº¦ã€‚")
    parser.add_argument("--top-p", type=float, default=1.0, help="ç”Ÿæˆtop-pã€‚")
    parser.add_argument("--max-new-tokens", type=int, default=131072, help="ç”Ÿæˆé•¿åº¦ã€‚")
    parser.add_argument("--dtype", default="auto", help="æ¨¡å‹dtypeï¼Œç”¨äºåˆå¹¶ç¯èŠ‚ã€‚")
    parser.add_argument("--trust-remote-code", action="store_true", help="æ˜¯å¦ä¿¡ä»»è¿œç¨‹ä»£ç ã€‚")
    parser.add_argument("--served-model-name", default="eval-model", help="vLLMå¯¹å¤–æš´éœ²çš„æ¨¡å‹åã€‚")
    parser.add_argument("--api-key", default="dummy", help="OpenAIå…¼å®¹æ¥å£çš„API Keyã€‚")
    parser.add_argument("--request-timeout", type=float, default=600.0, help="è¯·æ±‚å•æ¬¡è¶…æ—¶æ—¶é—´ã€‚")
    parser.add_argument("--max-samples", type=int, default=None, help="è°ƒè¯•ç”¨ï¼Œé™åˆ¶è¯„æµ‹æ ·æœ¬æ•°é‡ã€‚")
    parser.add_argument(
        "--max-num-request-per-dp",
        type=int,
        default=1,
        help="æ¯ä¸ªæ•°æ®å¹¶è¡Œï¼ˆDPï¼‰çš„vLLMåç«¯åŒæ—¶è¿è¡Œçš„è¯·æ±‚æ•°ä¸Šé™ã€‚",
    )

    args, unknown = parser.parse_known_args()
    vllm_args, leftover = extract_vllm_args(unknown)
    return args, vllm_args, leftover


def extract_vllm_args(unknown: List[str]) -> Tuple[List[str], List[str]]:
    vllm_args: List[str] = []
    leftover: List[str] = []
    idx = 0
    while idx < len(unknown):
        token = unknown[idx]
        if token.startswith("--vllm-"):
            stripped = "--" + token[len("--vllm-"):]
            if "=" in token:
                _, value = token.split("=", 1)
                vllm_args.extend([stripped, value])
            elif idx + 1 < len(unknown) and not unknown[idx + 1].startswith("-"):
                vllm_args.extend([stripped, unknown[idx + 1]])
                idx += 1
            else:
                vllm_args.append(stripped)
        else:
            leftover.append(token)
        idx += 1
    return vllm_args, leftover


def resolve_torch_dtype(dtype: Any) -> Any:
    """å°†dtypeå­—ç¬¦ä¸²è§£æä¸ºtorch.dtypeï¼Œæ”¯æŒauto/å¸¸è§åˆ«åï¼Œå…¼å®¹æ—§ç‰ˆTransformersç¼ºå°‘get_torch_dtypeçš„åœºæ™¯ã€‚"""
    if dtype is None:
        return None
    if isinstance(dtype, torch.dtype):
        return dtype
    if isinstance(dtype, str):
        normalized = dtype.lower()
        if normalized == "auto":
            return None
        mapping = {
            "float16": torch.float16,
            "fp16": torch.float16,
            "half": torch.float16,
            "bfloat16": torch.bfloat16,
            "bf16": torch.bfloat16,
            "float32": torch.float32,
            "fp32": torch.float32,
        }
        if normalized in mapping:
            return mapping[normalized]
    raise ValueError(f"ä¸æ”¯æŒçš„dtype: {dtype}")


def prepare_prompt(sample: Dict[str, Any]) -> str:
    """æ ¹æ®sampleæ„å»ºæ¨¡å‹è¾“å…¥promptï¼Œå¯æŒ‰éœ€ä¿®æ”¹å¢å¼ºã€‚"""
    if isinstance(sample, dict):
        if "prompt" in sample:
            return str(sample["prompt"])
        if "instruction" in sample and "input" in sample:
            return f"{sample['instruction']}\n{sample['input']}"
        if "instruction" in sample:
            return str(sample["instruction"])
        if "question" in sample:
            return str(sample["question"])
        if "text" in sample:
            return str(sample["text"])
    return str(sample)


def score_response(prompt: str, response: str, sample: Dict[str, Any]) -> float:
    """ç®€å•å ä½è¯„åˆ†ï¼šè‹¥sampleåŒ…å«answer/labelä¸”å‡ºç°åœ¨responseåˆ™è®°1ï¼Œå¦åˆ™0ã€‚"""
    answer = None
    if isinstance(sample, dict):
        answer = sample.get("answer") or sample.get("label")
    if answer is None:
        return 0.0
    return float(str(answer) in response)


def merge_model_if_needed(args: argparse.Namespace, result_dir: Path, logger: logging.Logger) -> Path:
    if not args.adapter:
        logger.info("æœªæä¾›adapterï¼Œç›´æ¥ä½¿ç”¨åŸºç¡€æ¨¡å‹ï¼š%s", args.model)
        return Path(args.model)

    output_dir = result_dir / "model"
    if output_dir.exists() and any(output_dir.iterdir()):
        logger.info("æ£€æµ‹åˆ°å·²å­˜åœ¨çš„åˆå¹¶æ¨¡å‹ç›®å½•ï¼Œç›´æ¥å¤ç”¨ï¼š%s", output_dir)
        return output_dir

    torch_dtype = resolve_torch_dtype(args.dtype)
    logger.info("åŠ è½½åŸºç¡€æ¨¡å‹ï¼š%s", args.model)
    model = AutoModelForCausalLM.from_pretrained(
        args.model,
        torch_dtype=torch_dtype,
        device_map="cpu",
        trust_remote_code=args.trust_remote_code,
    )
    logger.info("åŠ è½½åˆ†è¯å™¨ï¼š%s", args.model)
    tokenizer = AutoTokenizer.from_pretrained(
        args.model,
        trust_remote_code=args.trust_remote_code,
    )
    logger.info("åŠ è½½LoRA/PEFT adapterï¼š%s", args.adapter)
    model = PeftModel.from_pretrained(model, args.adapter)
    logger.info("æ‰§è¡Œmerge_and_unloadï¼Œå°†LoRAæƒé‡å†™å…¥åŸºç¡€æ¨¡å‹ã€‚")
    model = model.merge_and_unload()

    logger.info("ä¿å­˜åˆå¹¶æ¨¡å‹è‡³ï¼š%s", output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    model.save_pretrained(output_dir)
    tokenizer.save_pretrained(output_dir)
    return output_dir


def build_vllm_command(
    model_path: Path, port: int, args: argparse.Namespace, vllm_args: List[str]
) -> List[str]:
    cmd = [
        sys.executable,
        "-m",
        "vllm.entrypoints.openai.api_server",
        "--model",
        str(model_path),
        "--served-model-name",
        args.served_model_name,
        "--port",
        str(port),
        "--tensor-parallel-size",
        str(args.tp_size),
    ]
    # å®ç°æ–¹æ¡ˆï¼šåœ¨æ„é€  vLLM å¯åŠ¨å‘½ä»¤æ—¶è¿½åŠ  --gpu-memory-utilization å‚æ•°ï¼Œé»˜è®¤ 0.95ï¼Œå¯é€šè¿‡å‘½ä»¤è¡Œè¦†ç›–ã€‚
    if args.gpu_memory_utilization is not None:
        cmd.extend(["--gpu-memory-utilization", str(args.gpu_memory_utilization)])
    if args.trust_remote_code:
        cmd.append("--trust-remote-code")
    cmd.extend(vllm_args)
    return cmd


def pipe_to_logger(stream: Iterable[str], logger: logging.Logger, level: int, prefix: str) -> None:
    for line in stream:
        logger.log(level, "%s%s", prefix, line.rstrip("\n"))


def start_vllm_processes(
    model_path: Path, args: argparse.Namespace, vllm_args: List[str], logger: logging.Logger
) -> Tuple[List[subprocess.Popen], List[int]]:
    ports: List[int] = []
    processes: List[subprocess.Popen] = []
    env = os.environ.copy()
    dp_size = max(1, args.dp_size)

    for rank in range(dp_size):
        # è®¡ç®—å½“å‰è¿›ç¨‹åˆ†é…çš„GPU IDèŒƒå›´
        start_gpu_id = rank * args.tp_size
        end_gpu_id = start_gpu_id + args.tp_size
        gpu_ids = list(range(start_gpu_id, end_gpu_id))
        
        # æ ¡éªŒæ˜¯å¦è¶Šç•Œï¼ˆåŸºäºargs.num_gpusæˆ–è€…ç®€å•çš„é€»è¾‘æ ¡éªŒï¼Œè¿™é‡Œå‡è®¾ç”¨æˆ·é…ç½®æ­£ç¡®ï¼‰
        # å¦‚æœéœ€è¦æ›´ä¸¥æ ¼æ ¡éªŒï¼Œå¯ä»¥åœ¨æ­¤å¤„æ·»åŠ ã€‚
        
        env_local = env.copy()
        env_local["CUDA_VISIBLE_DEVICES"] = ",".join(map(str, gpu_ids))
        
        port = args.serve_port + rank
        cmd = build_vllm_command(model_path, port, args, vllm_args)
        logger.info("å¯åŠ¨vLLMåç«¯[%d/%d]ï¼Œç«¯å£%dï¼ŒGPUs=%sï¼Œå‘½ä»¤ï¼š%s", rank + 1, dp_size, port, gpu_ids, " ".join(cmd))
        proc = subprocess.Popen(
            cmd,
            env=env_local,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True,
            bufsize=1,
            preexec_fn=os.setsid,
        )
        processes.append(proc)
        ports.append(port)
        if proc.stdout:
            threading.Thread(
                target=pipe_to_logger,
                args=(proc.stdout, logger, logging.INFO, f"[vllm:{port}] "),
                daemon=True,
            ).start()
        if proc.stderr:
            threading.Thread(
                target=pipe_to_logger,
                args=(proc.stderr, logger, logging.ERROR, f"[vllm:{port}] "),
                daemon=True,
            ).start()
    return processes, ports


def stop_vllm_processes(processes: List[subprocess.Popen], logger: logging.Logger) -> None:
    for proc in processes:
        if proc.poll() is None:
            try:
                logger.info("å°è¯•ç»ˆæ­¢vLLMè¿›ç¨‹(pid=%d)ã€‚", proc.pid)
                os.killpg(os.getpgid(proc.pid), signal.SIGTERM)
            except Exception as exc:  # noqa: BLE001
                logger.warning("ç»ˆæ­¢è¿›ç¨‹(pid=%d)æ—¶å‘ç”Ÿå¼‚å¸¸ï¼š%s", proc.pid, exc)
    for proc in processes:
        if proc.poll() is None:
            try:
                proc.wait(timeout=10)
            except Exception:  # noqa: BLE001
                try:
                    os.killpg(os.getpgid(proc.pid), signal.SIGKILL)
                except Exception:
                    pass


def wait_for_vllm_ready(port: int, process: subprocess.Popen, timeout: float, logger: logging.Logger) -> bool:
    deadline = time.time() + timeout
    url = f"http://127.0.0.1:{port}/health"
    while time.time() < deadline:
        if process.poll() is not None:
            logger.error("vLLMè¿›ç¨‹(pid=%d)æå‰é€€å‡ºã€‚", process.pid)
            return False
        try:
            with urllib.request.urlopen(url, timeout=5) as resp:
                if resp.status == 200:
                    logger.info("ç«¯å£%dçš„vLLMå·²å°±ç»ªã€‚", port)
                    return True
        except Exception:
            time.sleep(2)
    logger.error("ç­‰å¾…ç«¯å£%dçš„vLLMè¶…æ—¶ã€‚", port)
    return False


def load_dataset_by_name(name: str, split: str):
    if ":" in name:
        path, subset = name.split(":", 1)
        return load_dataset(path, subset, split=split)
    return load_dataset(name, split=split)


def load_task_module(task_abbr: str) -> Any:
    """
    æ ¹æ®ä»»åŠ¡ç¼©å†™åŠ¨æ€åŠ è½½å¯¹åº”çš„ä»»åŠ¡æ¨¡å—ã€‚
    å®ç°æ–¹æ¡ˆï¼šæ ¹æ®ç¼©å†™ï¼ˆå¦‚aime2024ï¼‰åŠ¨æ€åŠ è½½tasks/{aime2024}.pyè„šæœ¬ä¸­çš„å‡½æ•°ã€‚
    ç®€åŒ–å®ç°ï¼šå°†é¡¹ç›®æ ¹ç›®å½•æ·»åŠ åˆ°sys.pathï¼Œä½¿ç›¸å¯¹å¯¼å…¥èƒ½å¤Ÿæ­£å¸¸å·¥ä½œã€‚
    
    Args:
        task_abbr: ä»»åŠ¡ç¼©å†™ï¼Œå¦‚ "aime2024"
    
    Returns:
        åŠ è½½çš„ä»»åŠ¡æ¨¡å—å¯¹è±¡
    
    Raises:
        FileNotFoundError: å¦‚æœä»»åŠ¡æ–‡ä»¶ä¸å­˜åœ¨
        ImportError: å¦‚æœæ¨¡å—åŠ è½½å¤±è´¥
    """
    # è·å–å½“å‰è„šæœ¬æ‰€åœ¨ç›®å½•å’Œé¡¹ç›®æ ¹ç›®å½•
    current_dir = Path(__file__).parent  # scripts/eval
    project_root = current_dir.parent.parent  # é¡¹ç›®æ ¹ç›®å½•ï¼ˆåŒ…å«scriptsçš„ç›®å½•ï¼‰
    task_file = current_dir / "tasks" / f"{task_abbr}.py"
    
    if not task_file.exists():
        raise FileNotFoundError(f"ä»»åŠ¡æ–‡ä»¶ä¸å­˜åœ¨: {task_file}")
    
    # å°†é¡¹ç›®æ ¹ç›®å½•æ·»åŠ åˆ°sys.pathï¼ˆå¦‚æœè¿˜æ²¡æœ‰çš„è¯ï¼‰ï¼Œè¿™æ ·ç›¸å¯¹å¯¼å…¥å°±èƒ½æ­£å¸¸å·¥ä½œ
    project_root_str = str(project_root)
    if project_root_str not in sys.path:
        sys.path.insert(0, project_root_str)
    
    # ä½¿ç”¨importlibåŠ¨æ€åŠ è½½æ¨¡å—
    module_name = f"scripts.eval.tasks.{task_abbr}"
    spec = importlib.util.spec_from_file_location(module_name, task_file)
    if spec is None or spec.loader is None:
        raise ImportError(f"æ— æ³•åŠ è½½ä»»åŠ¡æ¨¡å—: {task_file}")
    
    module = importlib.util.module_from_spec(spec)
    # è®¾ç½®__package__å±æ€§ï¼Œä½¿ç›¸å¯¹å¯¼å…¥èƒ½å¤Ÿæ­£å¸¸å·¥ä½œ
    module.__package__ = "scripts.eval.tasks"
    module.__name__ = module_name
    
    # æ‰§è¡Œæ¨¡å—ä»£ç 
    spec.loader.exec_module(module)
    
    # éªŒè¯å¿…éœ€çš„å‡½æ•°æ˜¯å¦å­˜åœ¨
    required_functions = ["load_dataset", "prepare_prompt", "score_response"]
    for func_name in required_functions:
        if not hasattr(module, func_name):
            raise ImportError(f"ä»»åŠ¡æ¨¡å— {task_abbr} ç¼ºå°‘å¿…éœ€çš„å‡½æ•°: {func_name}")
    
    return module


def generate_with_vllm(prompt: str, port: int, args: argparse.Namespace) -> str:
    """åŒæ­¥ç‰ˆæœ¬çš„vLLMç”Ÿæˆå‡½æ•°ï¼ˆä¿ç•™ç”¨äºå‘åå…¼å®¹ï¼‰ã€‚"""
    url = f"http://127.0.0.1:{port}/v1/chat/completions"
    payload = {
        "model": args.served_model_name,
        "messages": [{"role": "user", "content": prompt}],
        "temperature": args.temperature,
        "top_p": args.top_p,
        "max_tokens": args.max_new_tokens,
        "n": 1,
    }
    data = json.dumps(payload).encode("utf-8")
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {args.api_key}",
    }
    request = urllib.request.Request(url, data=data, headers=headers, method="POST")
    try:
        with urllib.request.urlopen(request, timeout=args.request_timeout) as response:
            body = response.read().decode("utf-8")
            content = json.loads(body)
    except urllib.error.HTTPError as exc:
        raise RuntimeError(f"vLLMè¿”å›HTTPé”™è¯¯: {exc}") from exc
    except urllib.error.URLError as exc:
        raise RuntimeError(f"vLLMè¿æ¥å¤±è´¥: {exc}") from exc

    try:
        return content["choices"][0]["message"]["content"]
    except Exception as exc:  # noqa: BLE001
        raise RuntimeError(f"è§£ævLLMå“åº”å¤±è´¥: {content}") from exc


async def generate_with_vllm_async(
    session: aiohttp.ClientSession, prompt: str, port: int, args: argparse.Namespace
) -> str:
    """å¼‚æ­¥ç‰ˆæœ¬çš„vLLMç”Ÿæˆå‡½æ•°ï¼Œç”¨äºå¹¶å‘è¯·æ±‚ã€‚"""
    url = f"http://127.0.0.1:{port}/v1/chat/completions"
    payload = {
        "model": args.served_model_name,
        "messages": [{"role": "user", "content": prompt}],
        "temperature": args.temperature,
        "top_p": args.top_p,
        "max_tokens": args.max_new_tokens,
        "n": 1,
    }
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {args.api_key}",
    }
    timeout = aiohttp.ClientTimeout(total=args.request_timeout)
    try:
        async with session.post(url, json=payload, headers=headers, timeout=timeout) as response:
            if response.status != 200:
                raise RuntimeError(f"vLLMè¿”å›HTTPé”™è¯¯: {response.status}")
            content = await response.json()
    except aiohttp.ClientError as exc:
        raise RuntimeError(f"vLLMè¿æ¥å¤±è´¥: {exc}") from exc

    try:
        return content["choices"][0]["message"]["content"]
    except Exception as exc:  # noqa: BLE001
        raise RuntimeError(f"è§£ævLLMå“åº”å¤±è´¥: {content}") from exc


def save_text(path: Path, text: str) -> None:
    path.parent.mkdir(parents=True, exist_ok=True)
    path.write_text(text, encoding="utf-8")


async def evaluate_dataset(
    dataset_name: str,
    task_module: Any,
    args: argparse.Namespace,
    ports: List[int],
    logger: logging.Logger,
) -> List[Dict[str, Any]]:
    """
    å¼‚æ­¥å¹¶å‘è¯„ä¼°æ•°æ®é›†ã€‚
    å®ç°æ–¹æ¡ˆï¼šä¸ºæ¯ä¸ªDPç«¯å£ç»´æŠ¤ä¸€ä¸ªä¿¡å·é‡ï¼ˆSemaphoreï¼‰é™åˆ¶å¹¶å‘æ•°ï¼Œåˆ›å»ºæ‰€æœ‰ä»»åŠ¡åå¼‚æ­¥æ‰§è¡Œï¼Œ
    å½“ä¸€ä¸ªè¯·æ±‚å®Œæˆæ—¶è‡ªåŠ¨ä»é˜Ÿåˆ—ä¸­å–å‡ºä¸‹ä¸€ä¸ªè¯·æ±‚å‘é€ï¼Œç¡®ä¿æ¯ä¸ªDPçš„å¹¶å‘æ•°ä¸è¶…è¿‡max_num_request_per_dpã€‚
    """
    dataset_dir = Path(args.result_dir) / dataset_name
    outputs_dir = dataset_dir / "outputs"
    result_file = dataset_dir / "result.jsonl"

    if result_file.exists():
        logger.warning("æ£€æµ‹åˆ°å·²å­˜åœ¨çš„ç»“æœæ–‡ä»¶ï¼Œè·³è¿‡é‡æ–°è¯„æµ‹æ•°æ®é›† %s : %s", dataset_name, result_file)
        try:
            with result_file.open("r", encoding="utf-8") as f:
                return [json.loads(line) for line in f if line.strip()]
        except Exception as exc:  # noqa: BLE001
            logger.error("è¯»å–å·²æœ‰ç»“æœå¤±è´¥ï¼Œå°†é‡æ–°è¯„æµ‹ã€‚é”™è¯¯ï¼š%s", exc)

    # ä½¿ç”¨ä»»åŠ¡æ¨¡å—ä¸­çš„load_datasetå‡½æ•°åŠ è½½æ•°æ®é›†
    ds = task_module.load_dataset_from_hf()

    # ä¸ºæ¯ä¸ªDPç«¯å£åˆ›å»ºä¿¡å·é‡ï¼Œé™åˆ¶å¹¶å‘è¯·æ±‚æ•°
    max_concurrent_per_dp = max(1, args.max_num_request_per_dp)
    semaphores: Dict[int, asyncio.Semaphore] = {port: asyncio.Semaphore(max_concurrent_per_dp) for port in ports}
    logger.info("æ¯ä¸ªDPç«¯å£çš„æœ€å¤§å¹¶å‘è¯·æ±‚æ•°ï¼š%d", max_concurrent_per_dp)

    # æ”¶é›†æ‰€æœ‰éœ€è¦å¤„ç†çš„ä»»åŠ¡
    # (problem_id, rollout_id, prompt, output_path, port_idx, sample)
    tasks_to_process: List[Tuple[int, int, str, Path, int, Dict[str, Any]]] = []
    cached_count = 0
    ports_cycle = len(ports)

    for idx, sample in enumerate(ds):
        # ä½¿ç”¨ä»»åŠ¡æ¨¡å—ä¸­çš„prepare_promptå‡½æ•°
        prompt = task_module.prepare_prompt(sample)
        problem_dir = outputs_dir / f"{idx:06d}"
        for rollout_id in range(args.rollout_n):
            output_path = problem_dir / f"rollout_{rollout_id:03d}.txt"
            port_idx = (idx * args.rollout_n + rollout_id) % ports_cycle
            if output_path.exists() and output_path.stat().st_size > 0:
                cached_count += 1
            tasks_to_process.append((idx, rollout_id, prompt, output_path, port_idx, sample))

    logger.info(
        "éœ€è¦å¤„ç†çš„è¯·æ±‚æ€»æ•°ï¼š%dï¼ˆå·²å­˜åœ¨ç¼“å­˜ï¼š%dï¼Œéœ€æ–°ç”Ÿæˆï¼š%dï¼‰",
        len(tasks_to_process),
        cached_count,
        len(tasks_to_process) - cached_count,
    )

    # å¼‚æ­¥ç”Ÿæˆå‡½æ•°
    async def generate_one_task(
        problem_id: int,
        rollout_id: int,
        prompt: str,
        output_path: Path,
        port_idx: int,
        sample: Dict[str, Any],
        session: aiohttp.ClientSession,
    ) -> Dict[str, Any]:
        response = ""
        # ç”¨æˆ·è¦æ±‚ï¼šè‹¥è¾“å‡ºæ–‡ä»¶å­˜åœ¨åˆ™åœ¨æ­¤å¤„ç›´æ¥å¤ç”¨å¹¶è·³è¿‡generate_with_vllm_asyncï¼Œé¿å…åœ¨ä¸»å¾ªç¯é‡å¤å†™è¯„åˆ†é€»è¾‘ã€‚
        if output_path.exists() and output_path.stat().st_size > 0:
            response = output_path.read_text(encoding="utf-8")
            logger.info("å¤ç”¨ç¼“å­˜ç»“æœï¼š%s", output_path)
        else:
            port = ports[port_idx]
            semaphore = semaphores[port]
            async with semaphore:  # é™åˆ¶æ¯ä¸ªDPçš„å¹¶å‘æ•°
                try:
                    logger.info("å‘ç«¯å£%dè¯·æ±‚ç”Ÿæˆï¼Œproblem=%06d rollout=%03d", port, problem_id, rollout_id)
                    response = await generate_with_vllm_async(session, prompt, port, args)
                    # å®ç°æ–¹æ¡ˆï¼šåœ¨è°ƒç”¨score_responseä¹‹å‰å…ˆä¿å­˜å“åº”åˆ°æ–‡ä»¶ï¼Œç¡®ä¿å³ä½¿score_responseæŠ¥é”™ä¹Ÿèƒ½ä¿ç•™å“åº”
                    save_text(output_path, response)
                except Exception as exc:  # noqa: BLE001
                    logger.error("ç”Ÿæˆå“åº”å¤±è´¥ problem=%06d rollout=%03d port=%d: %s", problem_id, rollout_id, port, exc)
                    # å¦‚æœç”Ÿæˆå¤±è´¥ï¼Œresponseä¸ºç©ºå­—ç¬¦ä¸²ï¼Œä½†ä¹Ÿè¦ä¿å­˜ï¼ˆå¯èƒ½æ˜¯ç©ºæ–‡ä»¶ï¼‰
                    if response:
                        save_text(output_path, response)
                    return {
                        "problem_id": problem_id,
                        "rollout_id": rollout_id,
                        "prompt": prompt,
                        "response": response,
                        "score": 0.0,
                        "details": {},
                    }
        
        # å“åº”å·²ä¿å­˜æˆ–æ¥è‡ªç¼“å­˜ï¼Œç°åœ¨å°è¯•è¯„åˆ†
        score = 0.0
        details = {}
        try:
            # ä½¿ç”¨ä»»åŠ¡æ¨¡å—ä¸­çš„score_responseå‡½æ•°
            score_result = task_module.score_response(prompt, response, sample)
            # å…¼å®¹è¿”å›å…ƒç»„æˆ–å•ä¸ªå€¼çš„æƒ…å†µ
            if isinstance(score_result, tuple):
                score, details = score_result
            else:
                score = score_result
                details = {}
        except Exception as exc:  # noqa: BLE001
            logger.warning("è¯„åˆ†å¤±è´¥ problem=%06d rollout=%03dï¼Œå“åº”å·²ä¿å­˜ï¼Œä½¿ç”¨é»˜è®¤åˆ†æ•°ã€‚é”™è¯¯ï¼š%s", problem_id, rollout_id, exc)
        
        return {
            "problem_id": problem_id,
            "rollout_id": rollout_id,
            "prompt": prompt,
            "response": response,
            "score": score,
            "details": details,
        }

    # åˆ›å»ºaiohttpä¼šè¯å¹¶å¹¶å‘æ‰§è¡Œæ‰€æœ‰ä»»åŠ¡
    async with aiohttp.ClientSession() as session:
        tasks = [
            generate_one_task(problem_id, rollout_id, prompt, output_path, port_idx, sample, session)
            for problem_id, rollout_id, prompt, output_path, port_idx, sample in tasks_to_process
        ]
        records = await asyncio.gather(*tasks)

    # æŒ‰problem_idå’Œrollout_idæ’åºï¼Œç¡®ä¿ç»“æœé¡ºåºä¸€è‡´
    records.sort(key=lambda x: (x["problem_id"], x["rollout_id"]))

    result_file.parent.mkdir(parents=True, exist_ok=True)
    with result_file.open("w", encoding="utf-8") as f:
        for record in records:
            f.write(json.dumps(record, ensure_ascii=False) + "\n")
    logger.info("æ•°æ®é›† %s è¯„æµ‹å®Œæˆï¼Œç»“æœå†™å…¥ %s", dataset_name, result_file)
    return records


def compute_pass_at_k(num_samples: int, num_correct: int, k: int) -> float:
    if num_correct == 0:
        return 0.0
    if num_samples <= k:
        return 1.0 if num_correct > 0 else 0.0
    return 1 - (math.comb(num_samples - num_correct, k) / math.comb(num_samples, k))


def compute_metrics(records: List[Dict[str, Any]], rollout_n: int) -> Dict[str, Dict[int, float]]:
    by_problem: Dict[int, List[float]] = {}
    for rec in records:
        by_problem.setdefault(int(rec["problem_id"]), []).append(float(rec["score"]))

    avg_at_k: Dict[int, float] = {}
    pass_at_k: Dict[int, float] = {}

    for k in range(1, rollout_n + 1):
        avg_scores = []
        pass_scores = []
        for scores in by_problem.values():
            sorted_scores = sorted(scores, reverse=True)
            topk = sorted_scores[:k]
            if topk:
                avg_scores.append(sum(topk) / len(topk))
            c = sum(1 for s in scores if s > 0)
            pass_scores.append(compute_pass_at_k(len(scores), c, k))

        avg_at_k[k] = sum(avg_scores) / len(avg_scores) if avg_scores else 0.0
        pass_at_k[k] = sum(pass_scores) / len(pass_scores) if pass_scores else 0.0

    return {"avg_at_k": avg_at_k, "pass_at_k": pass_at_k}


def main() -> None:
    args, vllm_args, leftover = parse_args()
    logger = setup_logging(Path(args.result_dir))
    if leftover:
        logger.warning("æ£€æµ‹åˆ°æ— æ³•è¯†åˆ«çš„å‚æ•°ï¼ˆå°†è¢«å¿½ç•¥ï¼‰ï¼š%s", leftover)

    with StageContext(logger, 1, "å‡†å¤‡æ¨¡å‹/åˆå¹¶LoRA"):
        model_path = merge_model_if_needed(args, Path(args.result_dir), logger)

    with StageContext(logger, 2, "å¯åŠ¨vLLMåç«¯"):
        processes, ports = start_vllm_processes(model_path, args, vllm_args, logger)
        atexit.register(stop_vllm_processes, processes, logger)

        def handle_signal(signum, frame):  # noqa: ANN001
            logger.warning("æ”¶åˆ°ä¿¡å·%dï¼Œå‡†å¤‡æ¸…ç†åé€€å‡ºã€‚", signum)
            stop_vllm_processes(processes, logger)
            sys.exit(1)

        signal.signal(signal.SIGINT, handle_signal)
        signal.signal(signal.SIGTERM, handle_signal)

        for proc, port in zip(processes, ports):
            if not wait_for_vllm_ready(port, proc, timeout=300, logger=logger):
                stop_vllm_processes(processes, logger)
                sys.exit(1)

    all_records: Dict[str, List[Dict[str, Any]]] = {}
    datasets_to_run = [item.strip() for item in args.dataset.split(",") if item.strip()]
    with StageContext(logger, 3, "æ•°æ®é›†è¯„æµ‹ä¸ç¼“å­˜/ç”Ÿæˆ"):
        async def run_evaluations():
            for task_abbr in datasets_to_run:
                logger.info("ğŸ§ª å¼€å§‹è¯„æµ‹æ•°æ®é›†ï¼š%s", task_abbr)
                try:
                    # åŠ¨æ€åŠ è½½ä»»åŠ¡æ¨¡å—
                    task_module = load_task_module(task_abbr)
                    logger.info("âœ… æˆåŠŸåŠ è½½ä»»åŠ¡æ¨¡å—ï¼š%s", task_abbr)
                except Exception as exc:  # noqa: BLE001
                    logger.error("âŒ åŠ è½½ä»»åŠ¡æ¨¡å—å¤±è´¥ %s: %s", task_abbr, exc)
                    raise exc
                # ä½¿ç”¨ä»»åŠ¡æ¨¡å—è¿›è¡Œè¯„æµ‹
                records = await evaluate_dataset(task_abbr, task_module, args, ports, logger)
                all_records[task_abbr] = records
                logger.info("âœ… å®Œæˆè¯„æµ‹æ•°æ®é›†ï¼š%s", task_abbr)
        
        asyncio.run(run_evaluations())

    with StageContext(logger, 4, "ç»Ÿè®¡é˜¶æ®µï¼šè®¡ç®—avg@kä¸pass@k"):
        overall_records: List[Dict[str, Any]] = []
        for name, records in all_records.items():
            overall_records.extend(records)
            metrics = compute_metrics(records, args.rollout_n)
            logger.info("ğŸ“Š æ•°æ®é›†%s avg@k: %s", name, metrics["avg_at_k"])
            logger.info("ğŸ“ˆ æ•°æ®é›†%s pass@k: %s", name, metrics["pass_at_k"])

        overall_metrics = compute_metrics(overall_records, args.rollout_n) if overall_records else None
        if overall_metrics:
            logger.info("ğŸŒ å…¨éƒ¨æ•°æ®é›†åˆå¹¶ avg@k: %s", overall_metrics["avg_at_k"])
            logger.info("ğŸŒŸ å…¨éƒ¨æ•°æ®é›†åˆå¹¶ pass@k: %s", overall_metrics["pass_at_k"])
        else:
            logger.warning("æœªè·å–åˆ°ä»»ä½•è®°å½•ï¼Œè·³è¿‡å…¨å±€ç»Ÿè®¡ã€‚")

    stop_vllm_processes(processes, logger)
    logger.info("å…¨éƒ¨è¯„æµ‹æµç¨‹å®Œæˆã€‚")


if __name__ == "__main__":
    main()
