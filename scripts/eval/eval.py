#!/usr/bin/env python3
# æœ¬è„šæœ¬ä¾æ®ç”¨æˆ·éœ€æ±‚ï¼šå®ç°è¯„æµ‹æµç¨‹ï¼ˆå‚æ•°è§£æã€æ¨¡å‹åˆå¹¶ã€å¯åŠ¨vLLMã€ç”Ÿæˆã€æ‰“åˆ†ã€ç¼“å­˜/æ¢å¤ã€æ—¥å¿—ã€é˜¶æ®µåŒ–æç¤ºã€æœ€ç»ˆç»Ÿè®¡ï¼‰ã€‚
# å®ç°æ–¹æ¡ˆï¼šä½¿ç”¨argparseè§£æå¸¸è§„ä¸--vllm-*é€ä¼ å‚æ•°ï¼Œå¿…è¦æ—¶åœ¨CPUä¸Šåˆå¹¶LoRAå¹¶ä¿å­˜ï¼›åå°å¯åŠ¨æ”¯æŒæ•°æ®å¹¶è¡Œçš„vLLMæœåŠ¡å™¨ï¼Œ
# è½®è¯¢åç«¯ç”Ÿæˆå¤šæ¬¡rolloutå¹¶ç¼“å­˜åˆ°æ–‡ä»¶ï¼Œéšåè°ƒç”¨score_responseæ±‡æ€»ä¸ºresult.jsonlï¼Œæœ€åæ–°å¢ä¸€ä¸ªç»Ÿè®¡é˜¶æ®µè¾“å‡ºavg@k/pass@kï¼Œ
# åŒæ—¶è®°å½•æ—¥å¿—å¹¶å°†stdout/stderrå†™å…¥latest_run.logï¼›é€šè¿‡é˜¶æ®µåŒ–æ—¥å¿—æ ‡æ˜ç¬¬å‡ é˜¶æ®µçš„å¼€å§‹/ç»“æŸï¼ˆå«emojiï¼‰ã€‚æœ¬ç‰ˆå¼ºåˆ¶ä¾èµ–vLLMä¸GPUï¼Œ
# æ–°å¢ --num-gpus å‚æ•°ç”¨äºè¿è¡Œå‰æ ¡éªŒå¯ç”¨ GPU æ•°ï¼ˆbash è„šæœ¬ä¸­è®¾ä¸º 8ï¼‰ï¼Œç¡®ä¿æŒ‰éœ€æ±‚ä½¿ç”¨å¤šå¡å¹¶è¡Œã€‚

import argparse
import asyncio
import atexit
import importlib.util
import json
import logging
import os
import signal
import subprocess
import sys
import threading
import time
import urllib.error
import urllib.request
import statistics
from pathlib import Path
from typing import Any, Dict, Iterable, List, Tuple, Optional, Set
import math

try:
    import aiohttp
except ImportError:
    raise ImportError("éœ€è¦å®‰è£… aiohttp: pip install aiohttp")

from datasets import load_dataset
from peft import PeftModel
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch


PROMPT_TEMPLATE = """{problem} Please reason step by step, and put your final answer within \\boxed{{}}."""
DATASETS = {
    "aime2024": ("HuggingFaceH4/aime_2024", "train"),
    "aime2025": ("yentinglin/aime_2025", "train"),
    "hmmt2025": ("FlagEval/HMMT_2025", "train"),
}


def load_dataset_from_hf(dataset_name: str):
    if dataset_name in DATASETS:
        hf_name, split = DATASETS[dataset_name]
        return load_dataset(hf_name, split=split)
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®é›†: {dataset_name}")


def prepare_prompt(dataset_name: str, sample: Dict[str, Any]) -> str:
    """æ ¹æ®sampleæ„å»ºæ¨¡å‹è¾“å…¥promptï¼Œå¯æŒ‰éœ€ä¿®æ”¹å¢å¼ºã€‚"""
    problem = None
    if "problem" in sample:
        problem = sample["problem"]
    elif "question" in sample:
        problem = sample["question"]
    elif "prompt" in sample:
        problem = sample["prompt"]
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ ·æœ¬: {sample}")
    return PROMPT_TEMPLATE.format(problem=problem)


os.environ["PYTHONPATH"] = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
from utils import grade_answer_verl


def score_response(dataset_name: str, response: str, sample: Dict[str, Any]) -> float:
    ground_truth = None
    if "answer" in sample:
        ground_truth = sample["answer"]
    elif "label" in sample:
        ground_truth = sample["label"]
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ ·æœ¬: {sample}")
    return 1.0 if grade_answer_verl(response, ground_truth) else 0.0


class StreamToLogger:
    """Redirect stdout/stderråˆ°loggerï¼Œç¡®ä¿è¾“å‡ºè¢«æ–‡ä»¶ä¸æ§åˆ¶å°åŒæ—¶è®°å½•ã€‚"""

    def __init__(self, logger: logging.Logger, level: int) -> None:
        self.logger = logger
        self.level = level
        self._buffer = ""

    def write(self, buffer: str) -> None:
        self._buffer += buffer
        while "\n" in self._buffer:
            line, self._buffer = self._buffer.split("\n", 1)
            self.logger.log(self.level, line)

    def flush(self) -> None:
        if self._buffer:
            self.logger.log(self.level, self._buffer)
            self._buffer = ""


def setup_logging(result_dir: Path) -> logging.Logger:
    result_dir.mkdir(parents=True, exist_ok=True)
    log_path = result_dir / "latest_run.log"

    for handler in logging.root.handlers[:]:
        logging.root.removeHandler(handler)

    logging.root.setLevel(logging.INFO)
    formatter = logging.Formatter("%(asctime)s | %(levelname)s | %(message)s")
    file_handler = logging.FileHandler(log_path, mode="w", encoding="utf-8")
    file_handler.setFormatter(formatter)
    console_handler = logging.StreamHandler(sys.__stdout__)
    console_handler.setFormatter(formatter)
    logging.root.addHandler(file_handler)
    logging.root.addHandler(console_handler)

    stdout_logger = logging.getLogger("stdout")
    stdout_logger.setLevel(logging.INFO)
    stdout_logger.propagate = True
    stderr_logger = logging.getLogger("stderr")
    stderr_logger.setLevel(logging.ERROR)
    stderr_logger.propagate = True
    sys.stdout = StreamToLogger(stdout_logger, logging.INFO)
    sys.stderr = StreamToLogger(stderr_logger, logging.ERROR)

    return logging.getLogger("eval_all")


class StageContext:
    """é˜¶æ®µåŒ–æ—¥å¿—ä¸Šä¸‹æ–‡ï¼Œæ ‡è®°å¼€å§‹/ç»“æŸå’Œå¤±è´¥åœºæ™¯ã€‚"""

    def __init__(
        self,
        logger: logging.Logger,
        stage_id: int | str,
        name: str,
        emoji_start: str = "ğŸš€",
        emoji_end: str = "ğŸ",
        emoji_fail: str = "ğŸ’¥",
    ) -> None:
        self.logger = logger
        self.stage_id = str(stage_id)
        self.name = name
        self.emoji_start = emoji_start
        self.emoji_end = emoji_end
        self.emoji_fail = emoji_fail

    def __enter__(self) -> "StageContext":
        self.logger.info(
            "%s ç¬¬%sé˜¶æ®µå¼€å§‹ï¼š%s", self.emoji_start, self.stage_id, self.name
        )
        return self

    def __exit__(self, exc_type, exc, tb) -> None:  # noqa: ANN001
        if exc_type is None:
            self.logger.info(
                "%s ç¬¬%sé˜¶æ®µç»“æŸï¼š%s", self.emoji_end, self.stage_id, self.name
            )
        else:
            self.logger.error(
                "%s ç¬¬%sé˜¶æ®µå¤±è´¥ï¼š%sï¼Œé”™è¯¯ï¼š%s",
                self.emoji_fail,
                self.stage_id,
                self.name,
                exc,
            )


def parse_args() -> Tuple[argparse.Namespace, List[str], List[str]]:
    parser = argparse.ArgumentParser(
        description="è¯„æµ‹å…¥å£è„šæœ¬ï¼Œæ”¯æŒæ¨¡å‹åˆå¹¶ã€vLLMå¯åŠ¨ä¸å¤šæ•°æ®é›†è¯„æµ‹ã€‚"
    )
    parser.add_argument("--result-dir", required=True, help="ä¸­é—´è¿‡ç¨‹ä¸ç»“æœè¾“å‡ºç›®å½•ã€‚")
    parser.add_argument("--model", required=True, help="åŸºç¡€æ¨¡å‹åç§°æˆ–è·¯å¾„ã€‚")
    parser.add_argument(
        "--adapter", default="", help="LoRA/PEFT adapterè·¯å¾„ï¼Œç•™ç©ºè¡¨ç¤ºä¸åˆå¹¶ã€‚"
    )
    parser.add_argument(
        "--dataset",
        default="aime2024",
        help="è¦è¯„æµ‹çš„æ•°æ®é›†ç¼©å†™ï¼Œè‹±æ–‡é€—å·åˆ†éš”ï¼ˆå¦‚ï¼šaime2024ï¼‰ã€‚",
    )
    parser.add_argument(
        "--rollout-n", type=int, default=1, help="æ¯ä¸ªsampleç”Ÿæˆå¤šå°‘æ¬¡rolloutã€‚"
    )
    parser.add_argument(
        "--serve-port", type=int, default=8000, help="ç¬¬ä¸€ä¸ªvLLMåç«¯ç«¯å£å·ã€‚"
    )
    parser.add_argument(
        "--dp-size", type=int, default=1, help="æ•°æ®å¹¶è¡Œåç«¯æ•°é‡ï¼ˆå¯åŠ¨å¤šä¸ªvLLMï¼‰ã€‚"
    )
    parser.add_argument(
        "--tp-size", type=int, default=1, help="ä¼ ç»™vLLMçš„å¼ é‡å¹¶è¡Œå¤§å°ã€‚"
    )
    parser.add_argument(
        "--num-gpus", type=int, default=1, help="è¿è¡Œå‰æ ¡éªŒéœ€è¦çš„GPUæ•°é‡ï¼Œä¸è¶³åˆ™æŠ¥é”™ã€‚"
    )
    parser.add_argument(
        "--gpu-memory-utilization",
        type=float,
        default=0.95,
        help="ä¼ ç»™vLLMçš„GPUæ˜¾å­˜åˆ©ç”¨ç‡ä¸Šé™ï¼ˆ0~1ï¼‰ï¼Œç”¨äºæ§åˆ¶å•å¡æ˜¾å­˜å ç”¨æ¯”ä¾‹ã€‚",
    )
    parser.add_argument("--temperature", type=float, default=1.0, help="ç”Ÿæˆæ¸©åº¦ã€‚")
    parser.add_argument("--top-p", type=float, default=1.0, help="ç”Ÿæˆtop-pã€‚")
    parser.add_argument("--max-new-tokens", type=int, default=131072, help="ç”Ÿæˆé•¿åº¦ã€‚")
    parser.add_argument("--dtype", default="auto", help="æ¨¡å‹dtypeï¼Œç”¨äºåˆå¹¶ç¯èŠ‚ã€‚")
    parser.add_argument(
        "--trust-remote-code", action="store_true", help="æ˜¯å¦ä¿¡ä»»è¿œç¨‹ä»£ç ã€‚"
    )
    parser.add_argument(
        "--served-model-name", default="eval-model", help="vLLMå¯¹å¤–æš´éœ²çš„æ¨¡å‹åã€‚"
    )
    parser.add_argument("--api-key", default="dummy", help="OpenAIå…¼å®¹æ¥å£çš„API Keyã€‚")
    parser.add_argument(
        "--request-timeout", type=float, default=600.0, help="è¯·æ±‚å•æ¬¡è¶…æ—¶æ—¶é—´ã€‚"
    )
    parser.add_argument(
        "--max-samples", type=int, default=None, help="è°ƒè¯•ç”¨ï¼Œé™åˆ¶è¯„æµ‹æ ·æœ¬æ•°é‡ã€‚"
    )
    parser.add_argument(
        "--max-num-request",
        type=int,
        default=None,
        help="æ¯ä¸ªæ•°æ®å¹¶è¡Œï¼ˆDPï¼‰çš„vLLMåç«¯åŒæ—¶è¿è¡Œçš„è¯·æ±‚æ•°ä¸Šé™ã€‚",
    )

    args, unknown = parser.parse_known_args()

    if args.max_num_request is None:
        args.max_num_request = args.dp_size
    else:
        assert args.max_num_request > 0
        assert args.max_num_request % args.dp_size == 0, (
            f"args.max_num_request({args.max_num_request}) must be divisible by args.dp_size({args.dp_size})"
        )

    vllm_args, leftover = extract_vllm_args(unknown)
    return args, vllm_args, leftover


def extract_vllm_args(unknown: List[str]) -> Tuple[List[str], List[str]]:
    vllm_args: List[str] = []
    leftover: List[str] = []
    idx = 0
    while idx < len(unknown):
        token = unknown[idx]
        if token.startswith("--vllm-"):
            stripped = "--" + token[len("--vllm-") :]
            if "=" in token:
                _, value = token.split("=", 1)
                vllm_args.extend([stripped, value])
            elif idx + 1 < len(unknown) and not unknown[idx + 1].startswith("-"):
                vllm_args.extend([stripped, unknown[idx + 1]])
                idx += 1
            else:
                vllm_args.append(stripped)
        else:
            leftover.append(token)
        idx += 1
    return vllm_args, leftover


def resolve_torch_dtype(dtype: Any) -> Any:
    """
    å°†dtypeå­—ç¬¦ä¸²è§£æä¸ºtorch.dtypeï¼Œæ”¯æŒauto/å¸¸è§åˆ«åï¼Œå…¼å®¹æ—§ç‰ˆTransformersç¼ºå°‘get_torch_dtypeçš„åœºæ™¯ã€‚
    """
    if dtype is None:
        return None
    if isinstance(dtype, torch.dtype):
        return dtype
    if isinstance(dtype, str):
        normalized = dtype.lower()
        if normalized == "auto":
            return None
        mapping = {
            "float16": torch.float16,
            "fp16": torch.float16,
            "half": torch.float16,
            "bfloat16": torch.bfloat16,
            "bf16": torch.bfloat16,
            "float32": torch.float32,
            "fp32": torch.float32,
        }
        if normalized in mapping:
            return mapping[normalized]
    raise ValueError(f"ä¸æ”¯æŒçš„dtype: {dtype}")


def merge_model_if_needed(
    args: argparse.Namespace, result_dir: Path, logger: logging.Logger
) -> Path:
    if not args.adapter:
        logger.info("æœªæä¾›adapterï¼Œç›´æ¥ä½¿ç”¨åŸºç¡€æ¨¡å‹ï¼š%s", args.model)
        return Path(args.model)

    output_dir = result_dir / "model"
    if output_dir.exists() and any(output_dir.iterdir()):
        logger.info("æ£€æµ‹åˆ°å·²å­˜åœ¨çš„åˆå¹¶æ¨¡å‹ç›®å½•ï¼Œç›´æ¥å¤ç”¨ï¼š%s", output_dir)
        return output_dir

    torch_dtype = resolve_torch_dtype(args.dtype)
    logger.info("åŠ è½½åŸºç¡€æ¨¡å‹ï¼š%s", args.model)
    model = AutoModelForCausalLM.from_pretrained(
        args.model,
        torch_dtype=torch_dtype,
        device_map="cpu",
        trust_remote_code=args.trust_remote_code,
    )
    logger.info("åŠ è½½åˆ†è¯å™¨ï¼š%s", args.model)
    tokenizer = AutoTokenizer.from_pretrained(
        args.model,
        trust_remote_code=args.trust_remote_code,
    )
    logger.info("åŠ è½½LoRA/PEFT adapterï¼š%s", args.adapter)
    model = PeftModel.from_pretrained(model, args.adapter)
    logger.info("æ‰§è¡Œmerge_and_unloadï¼Œå°†LoRAæƒé‡å†™å…¥åŸºç¡€æ¨¡å‹ã€‚")
    model = model.merge_and_unload()

    logger.info("ä¿å­˜åˆå¹¶æ¨¡å‹è‡³ï¼š%s", output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    model.save_pretrained(output_dir)
    tokenizer.save_pretrained(output_dir)
    return output_dir


def build_vllm_command(
    model_path: Path, port: int, args: argparse.Namespace, vllm_args: List[str]
) -> List[str]:
    cmd = [
        sys.executable,
        "-m",
        "vllm.entrypoints.openai.api_server",
        "--model",
        str(model_path),
        "--served-model-name",
        args.served_model_name,
        "--port",
        str(port),
        "--tensor-parallel-size",
        str(args.tp_size),
    ]
    # å®ç°æ–¹æ¡ˆï¼šåœ¨æ„é€  vLLM å¯åŠ¨å‘½ä»¤æ—¶è¿½åŠ  --gpu-memory-utilization å‚æ•°ï¼Œé»˜è®¤ 0.95ï¼Œå¯é€šè¿‡å‘½ä»¤è¡Œè¦†ç›–ã€‚
    if args.gpu_memory_utilization is not None:
        cmd.extend(["--gpu-memory-utilization", str(args.gpu_memory_utilization)])
    if args.trust_remote_code:
        cmd.append("--trust-remote-code")
    cmd.extend(vllm_args)
    return cmd


def pipe_to_logger(
    stream: Iterable[str], logger: logging.Logger, level: int, prefix: str
) -> None:
    for line in stream:
        logger.log(level, "%s%s", prefix, line.rstrip("\n"))


def start_vllm_processes(
    model_path: Path,
    args: argparse.Namespace,
    vllm_args: List[str],
    logger: logging.Logger,
) -> Tuple[List[subprocess.Popen], List[int]]:
    ports: List[int] = []
    processes: List[subprocess.Popen] = []
    env = os.environ.copy()
    dp_size = max(1, args.dp_size)

    for rank in range(dp_size):
        # è®¡ç®—å½“å‰è¿›ç¨‹åˆ†é…çš„GPU IDèŒƒå›´
        start_gpu_id = rank * args.tp_size
        end_gpu_id = start_gpu_id + args.tp_size
        gpu_ids = list(range(start_gpu_id, end_gpu_id))

        # æ ¡éªŒæ˜¯å¦è¶Šç•Œï¼ˆåŸºäºargs.num_gpusæˆ–è€…ç®€å•çš„é€»è¾‘æ ¡éªŒï¼Œè¿™é‡Œå‡è®¾ç”¨æˆ·é…ç½®æ­£ç¡®ï¼‰
        # å¦‚æœéœ€è¦æ›´ä¸¥æ ¼æ ¡éªŒï¼Œå¯ä»¥åœ¨æ­¤å¤„æ·»åŠ ã€‚

        env_local = env.copy()
        env_local["CUDA_VISIBLE_DEVICES"] = ",".join(map(str, gpu_ids))

        port = args.serve_port + rank
        cmd = build_vllm_command(model_path, port, args, vllm_args)
        logger.info(
            "å¯åŠ¨vLLMåç«¯[%d/%d]ï¼Œç«¯å£%dï¼ŒGPUs=%sï¼Œå‘½ä»¤ï¼š%s",
            rank + 1,
            dp_size,
            port,
            gpu_ids,
            " ".join(cmd),
        )
        proc = subprocess.Popen(
            cmd,
            env=env_local,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True,
            bufsize=1,
            preexec_fn=os.setsid,
        )
        processes.append(proc)
        ports.append(port)
        if proc.stdout:
            threading.Thread(
                target=pipe_to_logger,
                args=(proc.stdout, logger, logging.INFO, f"[vllm:{port}] "),
                daemon=True,
            ).start()
        if proc.stderr:
            threading.Thread(
                target=pipe_to_logger,
                args=(proc.stderr, logger, logging.ERROR, f"[vllm:{port}] "),
                daemon=True,
            ).start()
    return processes, ports


def stop_vllm_processes(
    processes: List[subprocess.Popen], logger: logging.Logger
) -> None:
    for proc in processes:
        if proc.poll() is None:
            try:
                logger.info("å°è¯•ç»ˆæ­¢vLLMè¿›ç¨‹(pid=%d)ã€‚", proc.pid)
                os.killpg(os.getpgid(proc.pid), signal.SIGTERM)
            except Exception as exc:  # noqa: BLE001
                logger.warning("ç»ˆæ­¢è¿›ç¨‹(pid=%d)æ—¶å‘ç”Ÿå¼‚å¸¸ï¼š%s", proc.pid, exc)
    for proc in processes:
        if proc.poll() is None:
            try:
                proc.wait(timeout=10)
            except Exception:  # noqa: BLE001
                try:
                    os.killpg(os.getpgid(proc.pid), signal.SIGKILL)
                except Exception:
                    pass


def wait_for_vllm_ready(
    port: int, process: subprocess.Popen, timeout: float, logger: logging.Logger
) -> bool:
    deadline = time.time() + timeout
    url = f"http://127.0.0.1:{port}/health"
    while time.time() < deadline:
        if process.poll() is not None:
            logger.error("vLLMè¿›ç¨‹(pid=%d)æå‰é€€å‡ºã€‚", process.pid)
            return False
        try:
            with urllib.request.urlopen(url, timeout=5) as resp:
                if resp.status == 200:
                    logger.info("ç«¯å£%dçš„vLLMå·²å°±ç»ªã€‚", port)
                    return True
        except Exception:
            time.sleep(2)
    logger.error("ç­‰å¾…ç«¯å£%dçš„vLLMè¶…æ—¶ã€‚", port)
    return False


def generate_with_vllm(prompt: str, port: int, args: argparse.Namespace) -> str:
    """åŒæ­¥ç‰ˆæœ¬çš„vLLMç”Ÿæˆå‡½æ•°ï¼ˆä¿ç•™ç”¨äºå‘åå…¼å®¹ï¼‰ã€‚"""
    url = f"http://127.0.0.1:{port}/v1/chat/completions"
    payload = {
        "model": args.served_model_name,
        "messages": [{"role": "user", "content": prompt}],
        "temperature": args.temperature,
        "top_p": args.top_p,
        "max_tokens": args.max_new_tokens,
        "n": 1,
    }
    data = json.dumps(payload).encode("utf-8")
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {args.api_key}",
    }
    request = urllib.request.Request(url, data=data, headers=headers, method="POST")
    try:
        with urllib.request.urlopen(request, timeout=args.request_timeout) as response:
            body = response.read().decode("utf-8")
            content = json.loads(body)
    except urllib.error.HTTPError as exc:
        raise RuntimeError(f"vLLMè¿”å›HTTPé”™è¯¯: {exc}") from exc
    except urllib.error.URLError as exc:
        raise RuntimeError(f"vLLMè¿æ¥å¤±è´¥: {exc}") from exc

    try:
        return content["choices"][0]["message"]["content"]
    except Exception as exc:  # noqa: BLE001
        raise RuntimeError(f"è§£ævLLMå“åº”å¤±è´¥: {content}") from exc


async def generate_with_vllm_async(
    session: aiohttp.ClientSession, prompt: str, port: int, args: argparse.Namespace
) -> str:
    """å¼‚æ­¥ç‰ˆæœ¬çš„vLLMç”Ÿæˆå‡½æ•°ï¼Œç”¨äºå¹¶å‘è¯·æ±‚ã€‚"""
    url = f"http://127.0.0.1:{port}/v1/chat/completions"
    payload = {
        "model": args.served_model_name,
        "messages": [{"role": "user", "content": prompt}],
        "temperature": args.temperature,
        "top_p": args.top_p,
        "max_tokens": args.max_new_tokens,
        "n": 1,
    }
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {args.api_key}",
    }
    timeout = aiohttp.ClientTimeout(total=args.request_timeout)
    try:
        async with session.post(
            url, json=payload, headers=headers, timeout=timeout
        ) as response:
            if response.status != 200:
                raise RuntimeError(f"vLLMè¿”å›HTTPé”™è¯¯: {response.status}")
            content = await response.json()
    except aiohttp.ClientError as exc:
        raise RuntimeError(f"vLLMè¿æ¥å¤±è´¥: {exc}") from exc

    try:
        return content["choices"][0]["message"]["content"]
    except Exception as exc:  # noqa: BLE001
        raise RuntimeError(f"è§£ævLLMå“åº”å¤±è´¥: {content}") from exc


class ProgressVisualizer:
    def __init__(
        self,
        filepath: Path,
        problem_n: int,
        rollout_n: int,
        completed: Set[Tuple[int, int]],
    ) -> None:
        self.filepath = filepath
        self.problem_n = problem_n
        self.rollout_n = rollout_n
        # è¡Œ: rollout_id, åˆ—: problem_id
        self.grid = [["." for _ in range(problem_n)] for _ in range(rollout_n)]
        for pid, rid in completed:
            if 0 <= rid < rollout_n and 0 <= pid < problem_n:
                self.grid[rid][pid] = "X"
        self.lock = asyncio.Lock()
        self._write_sync()

    def _write_sync(self) -> None:
        try:
            with self.filepath.open("w", encoding="utf-8") as f:
                for row in self.grid:
                    f.write("".join(row) + "\n")
        except Exception:
            pass

    async def update(self, problem_id: int, rollout_id: int) -> None:
        if 0 <= rollout_id < self.rollout_n and 0 <= problem_id < self.problem_n:
            async with self.lock:
                if self.grid[rollout_id][problem_id] != "X":
                    self.grid[rollout_id][problem_id] = "X"
                    await asyncio.get_running_loop().run_in_executor(
                        None, self._write_sync
                    )

    def cleanup(self) -> None:
        try:
            if self.filepath.exists():
                self.filepath.unlink()
        except Exception:
            pass


async def generate_responses(
    args: argparse.Namespace,
    dataset_name: str,
    rollout_n: int,
    ports: List[int],
    logger: logging.Logger,
) -> None:
    """
    å¼‚æ­¥å¹¶å‘ç”Ÿæˆå“åº”å¹¶å­˜å…¥output.jsonlã€‚
    å®ç°æ–¹æ¡ˆï¼šè¯»å–å·²æœ‰output.jsonlå»ºç«‹ç¼“å­˜ï¼Œä»…ç”Ÿæˆç¼ºå¤±çš„æ¡ç›®ã€‚
    ç”Ÿæˆç»“æœå®æ—¶è¿½åŠ å†™å…¥output.jsonlã€‚
    """
    dataset_dir = Path(args.result_dir) / dataset_name
    output_file = dataset_dir / "output.jsonl"
    dataset_dir.mkdir(parents=True, exist_ok=True)

    with StageContext(logger, "C.1", "è¯»å–ç¼“å­˜çš„è¾“å‡º"):
        generated_results: List[Dict[str, Any]] = []
        cache: Set[Tuple[int, int]] = set()

        if output_file.exists():
            with output_file.open("r", encoding="utf-8") as f:
                for line in f:
                    line = line.strip()
                    if not line:
                        continue
                    try:
                        data = json.loads(line)
                        if (
                            "problem_id" in data
                            and "rollout_id" in data
                            and "response" in data
                        ):
                            generated_results.append(data)
                            cache.add((data["problem_id"], data["rollout_id"]))
                    except json.JSONDecodeError:
                        logger.warning("output.jsonlä¸­å­˜åœ¨æ— æ•ˆJSONè¡Œï¼Œå·²è·³è¿‡ã€‚")

        logger.info("å·²åŠ è½½ç¼“å­˜æ¡ç›®æ•°ï¼š%d", len(generated_results))

    with StageContext(logger, "C.2", "å‡†å¤‡ç”Ÿæˆä»»åŠ¡"):
        ds = load_dataset_from_hf(dataset_name)
        max_concurrent_per_dp = max(1, args.max_num_request // args.dp_size)
        semaphores = {port: asyncio.Semaphore(max_concurrent_per_dp) for port in ports}

        tasks_to_process: List[Tuple[int, int, str, int]] = []
        ports_cycle = len(ports)

        for idx, sample in enumerate(ds):
            prompt = prepare_prompt(dataset_name, sample)
            for rollout_id in range(rollout_n):
                if (idx, rollout_id) in cache:
                    continue
                port_idx = (idx * rollout_n + rollout_id) % ports_cycle
                tasks_to_process.append((idx, rollout_id, prompt, port_idx))

        logger.info("éœ€è¦æ–°ç”Ÿæˆçš„è¯·æ±‚æ•°ï¼š%d", len(tasks_to_process))

        visualizer = ProgressVisualizer(
            dataset_dir / "process.txt", len(ds), rollout_n, cache
        )

        if not tasks_to_process:
            logger.info("æ‰€æœ‰è¯·æ±‚å·²åœ¨ç¼“å­˜ä¸­ï¼Œæ— éœ€ç”Ÿæˆã€‚")
            visualizer.cleanup()
            return

    with StageContext(logger, "C.3", "å¹¶è¡Œç”Ÿæˆ"):
        file_lock = asyncio.Lock()

        async def generate_one_task(
            problem_id: int,
            rollout_id: int,
            prompt: str,
            port_idx: int,
            session: aiohttp.ClientSession,
        ) -> None:
            port = ports[port_idx]
            semaphore = semaphores[port]
            response = ""

            async with semaphore:
                try:
                    logger.info(
                        "å‘ç«¯å£%dè¯·æ±‚ç”Ÿæˆï¼Œproblem=%06d rollout=%03d",
                        port,
                        problem_id,
                        rollout_id,
                    )
                    response = await generate_with_vllm_async(
                        session, prompt, port, args
                    )
                except Exception as exc:
                    logger.error(
                        "ç”Ÿæˆå¤±è´¥ problem=%06d rollout=%03d port=%d: %s",
                        problem_id,
                        rollout_id,
                        port,
                        exc,
                    )
                    response = ""

            record = {
                "problem_id": problem_id,
                "rollout_id": rollout_id,
                "response": response,
            }

            generated_results.append(record)

            async with file_lock:
                with output_file.open("a", encoding="utf-8") as f:
                    f.write(json.dumps(record, ensure_ascii=False) + "\n")

            await visualizer.update(problem_id, rollout_id)

        async with aiohttp.ClientSession() as session:
            tasks = [
                generate_one_task(pid, rid, pmt, pidx, session)
                for pid, rid, pmt, pidx in tasks_to_process
            ]
            await asyncio.gather(*tasks)
            visualizer.cleanup()

        logger.info("æ•°æ®é›† %s ç”Ÿæˆå®Œæˆï¼Œç»“æœå­˜å…¥ %s", dataset_name, output_file)


def evaluate_dataset_results(
    args: argparse.Namespace,
    dataset_name: str,
    rollout_n: int,
    logger: logging.Logger,
) -> Dict[str, Dict[int, float]]:
    """
    è¯„æµ‹é˜¶æ®µï¼šè¯»å–output.jsonlï¼Œè¯„åˆ†å¹¶ç”Ÿæˆresult.jsonlï¼Œè¿”å›ç»Ÿè®¡æŒ‡æ ‡ã€‚
    """
    dataset_dir = Path(args.result_dir) / dataset_name
    output_file = dataset_dir / "output.jsonl"
    result_file = dataset_dir / "result.jsonl"
    result_json_file = dataset_dir / "result.json"

    with StageContext(logger, "D.1", "åŠ è½½æ¨¡å‹è¾“å‡º"):
        if not output_file.exists():
            raise ValueError(f"æœªæ‰¾åˆ°output.jsonlï¼Œæ— æ³•è¿›è¡Œè¯„æµ‹ï¼š{dataset_name}")

        outputs_map: Dict[int, List[Tuple[int, str]]] = {}
        with output_file.open("r", encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if not line:
                    continue
                try:
                    d = json.loads(line)
                    if "problem_id" in d and "rollout_id" in d:
                        outputs_map.setdefault(d["problem_id"], []).append(
                            (d["rollout_id"], d.get("response", ""))
                        )
                except json.JSONDecodeError:
                    pass

    with StageContext(logger, "D.2", "åŠ è½½åŸæ•°æ®é›†"):
        ds = load_dataset_from_hf(dataset_name)

    with StageContext(logger, "D.3", "å¹¶è¡Œè¯„æµ‹&è®¡ç®—æŒ‡æ ‡"):
        records_for_metrics: List[Dict[str, Any]] = []
        raw_stats_list: List[Dict[str, Any]] = []

        with result_file.open("w", encoding="utf-8") as rf:
            for idx, sample in enumerate(ds):
                problem_id = idx
                prompt = prepare_prompt(dataset_name, sample)

                rollouts = outputs_map.get(problem_id, [])
                # æŒ‰rollout_idæ’åº
                rollouts.sort(key=lambda x: x[0])
                rollout_dict = {r[0]: r[1] for r in rollouts}

                responses = []
                scores = []

                for rid in range(rollout_n):
                    resp = rollout_dict.get(rid, "")
                    responses.append(resp)

                    if resp:
                        try:
                            s_res = score_response(dataset_name, resp, sample)
                            if isinstance(s_res, tuple):
                                score = float(s_res[0])
                            else:
                                score = float(s_res)
                        except Exception as e:
                            logger.warning("è¯„åˆ†å‡ºé”™ p=%d r=%d: %s", problem_id, rid, e)
                            score = 0.0
                    else:
                        score = 0.0
                    scores.append(score)

                    records_for_metrics.append(
                        {"problem_id": problem_id, "rollout_id": rid, "score": score}
                    )

                if scores:
                    avg_val = statistics.mean(scores)
                    max_val = max(scores)
                    min_val = min(scores)
                    mean_val = avg_val
                    try:
                        std_val = statistics.stdev(scores)
                    except statistics.StatisticsError:
                        std_val = 0.0
                else:
                    avg_val = max_val = min_val = mean_val = std_val = 0.0

                record = {
                    "problem_id": problem_id,
                    "prompt": prompt,
                    "responses": responses,
                    "scores": scores,
                    "avg": avg_val,
                    "max": max_val,
                    "min": min_val,
                    "mean": mean_val,
                    "std": std_val,
                }
                rf.write(json.dumps(record, ensure_ascii=False) + "\n")

                raw_stats_list.append(
                    {
                        "problem_id": problem_id,
                        "avg": avg_val,
                        "max": max_val,
                        "min": min_val,
                        "mean": mean_val,
                        "std": std_val,
                    }
                )

        if raw_stats_list:
            summary = {
                "avg": statistics.mean(x["avg"] for x in raw_stats_list),
                "max": statistics.mean(x["max"] for x in raw_stats_list),
                "min": statistics.mean(x["min"] for x in raw_stats_list),
                "mean": statistics.mean(x["mean"] for x in raw_stats_list),
                "std": statistics.mean(x["std"] for x in raw_stats_list),
            }
        else:
            summary = {"avg": 0.0, "max": 0.0, "min": 0.0, "mean": 0.0, "std": 0.0}

        final_json = {
            "dataset_name": dataset_name,
            "rollout_n": rollout_n,
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "summary": summary,
            "raw": raw_stats_list,
            "response_example": [
                outputs_map[0][0],
                outputs_map[-1][-1],
            ],
        }

        with result_json_file.open("w", encoding="utf-8") as f:
            json.dump(final_json, f, indent=2, ensure_ascii=False)

        logger.info("è¯„æµ‹å®Œæˆï¼Œç»“æœå†™å…¥ %s å’Œ %s", result_file, result_json_file)


async def main() -> None:
    args, vllm_args, leftover = parse_args()
    logger = setup_logging(Path(args.result_dir))
    if leftover:
        logger.warning("æ£€æµ‹åˆ°æ— æ³•è¯†åˆ«çš„å‚æ•°ï¼ˆå°†è¢«å¿½ç•¥ï¼‰ï¼š%s", leftover)

    with StageContext(logger, "A", "å‡†å¤‡æ¨¡å‹/åˆå¹¶LoRA"):
        model_path = merge_model_if_needed(args, Path(args.result_dir), logger)

    with StageContext(logger, "B", "å¯åŠ¨vLLMåç«¯"):
        processes, ports = start_vllm_processes(model_path, args, vllm_args, logger)
        atexit.register(stop_vllm_processes, processes, logger)

        def handle_signal(signum, frame):  # noqa: ANN001
            logger.warning("æ”¶åˆ°ä¿¡å·%dï¼Œå‡†å¤‡æ¸…ç†åé€€å‡ºã€‚", signum)
            stop_vllm_processes(processes, logger)
            sys.exit(1)

        signal.signal(signal.SIGINT, handle_signal)
        signal.signal(signal.SIGTERM, handle_signal)

        for proc, port in zip(processes, ports):
            if not wait_for_vllm_ready(port, proc, timeout=300, logger=logger):
                stop_vllm_processes(processes, logger)
                sys.exit(1)

    datasets_to_run = [item.strip() for item in args.dataset.split(",") if item.strip()]

    with StageContext(logger, "C", "æ•°æ®é›†ç”Ÿæˆï¼ˆç¼“å­˜/ç”Ÿæˆï¼‰"):
        for task_abbr in datasets_to_run:
            logger.info("ğŸš€ å¼€å§‹ç”Ÿæˆæ•°æ®é›†ï¼š%s", task_abbr)
            rollout_n = args.rollout_n
            if "@" in task_abbr:
                rollout_n = int(task_abbr.split("@")[1])
                task_abbr = task_abbr.split("@")[0]
            await generate_responses(args, task_abbr, rollout_n, ports, logger)
            logger.info("âœ… å®Œæˆç”Ÿæˆæ•°æ®é›†ï¼š%s (rollout=%d)", task_abbr, rollout_n)

    with StageContext(logger, "D", "è¯„æµ‹ä¸ç»Ÿè®¡"):
        for task_abbr in datasets_to_run:
            logger.info("ğŸ“Š å¼€å§‹è¯„æµ‹æ•°æ®é›†ï¼š%s", task_abbr)
            rollout_n = args.rollout_n
            if "@" in task_abbr:
                rollout_n = int(task_abbr.split("@")[1])
                task_abbr = task_abbr.split("@")[0]
            evaluate_dataset_results(args, task_abbr, rollout_n, logger)
            logger.info("ğŸ“Š æ•°æ®é›†%s (rollout=%d) è¯„æµ‹å®Œæˆ", task_abbr, rollout_n)

    stop_vllm_processes(processes, logger)
    logger.info("å…¨éƒ¨è¯„æµ‹æµç¨‹å®Œæˆã€‚")


if __name__ == "__main__":
    asyncio.run(main())
